"""Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ Ñ…Ð°Ð½Ð´Ð»ÐµÑ€Ñ‹ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ñ Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ñ„Ð°Ð¹Ð»Ð¾Ð².

Handles file uploads, processing, and analysis responses.
Supports multiple LLM providers with fallback.

UPDATED 2025-12-25 14:45:
- Ð£Ð”ÐÐ›Ð•ÐÐ« Ð´ÑƒÐ±Ð»Ð¸ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð¾Ð² Ð¸ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð²
- Ð’ÑÐµ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð² replicate_client.py
- See replicate_client.py for [LLM TEXT], [LLM SYSTEM PROMPT], [LLM USER PROMPT]
"""

import logging
import tempfile
import uuid
import base64
import asyncio
from pathlib import Path

from aiogram import Router, F
from aiogram.types import Message, Document, File
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State
from aiogram.filters import StateFilter
from aiogram.enums import ContentType
from aiogram.exceptions import TelegramNetworkError

from app.config import get_settings
from app.states.analysis import DocumentAnalysisStates
from app.states.homework import HomeworkStates
from app.states.conversation import ConversationStates
from app.states.prompts import PromptStates
from app.services.file_processing.converter import FileConverter
from app.services.llm.llm_factory import LLMFactory
from app.utils.text_splitter import TextSplitter
from app.utils.cleanup import CleanupManager

logger = logging.getLogger(__name__)

router = Router()
config = get_settings()

llm_factory = LLMFactory(
    primary_provider=config.LLM_PROVIDER,
    openai_api_key=config.OPENAI_API_KEY or None,
    openai_model=config.OPENAI_MODEL,
    replicate_api_token=config.REPLICATE_API_TOKEN or None,
    replicate_model=config.REPLICATE_MODEL,
)


@router.message(
    F.document,
    ~StateFilter(HomeworkStates, ConversationStates, PromptStates),
)
async def handle_document(
    message: Message,
    state: FSMContext,
) -> None:
    """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð² ÐžÐ‘Ð©Ð•Ðœ Ñ€ÐµÐ¶Ð¸Ð¼Ðµ.
    
    Args:
        message: User message with document
        state: FSM state
    """
    current_state = await state.get_state()
    user_id = message.from_user.id
    
    logger.debug(f"[DOCUMENTS DEBUG] User {user_id}: handle_document called")
    logger.debug(f"[DOCUMENTS DEBUG] User {user_id}: Current state: {current_state}")
    
    if not message.document:
        await message.answer("âŒ Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ Ð½Ðµ Ð·Ð°Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½.")
        return
    
    document: Document = message.document
    file_size = document.file_size or 0
    
    logger.info(f"documents.handle_document: User {user_id} uploading {document.file_name}")
    
    if file_size > config.MAX_FILE_SIZE:
        max_size_mb = config.MAX_FILE_SIZE / (1024 * 1024)
        await message.answer(
            f"âš ï¸ Ð¤Ð°Ð¹Ð» ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹: {file_size / (1024 * 1024):.1f} MB\n"
            f"ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼: {max_size_mb:.1f} MB",
        )
        return
    
    await state.set_state(DocumentAnalysisStates.processing)
    
    processing_msg = await message.answer(
        "ðŸ” ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚...\n"
        "Ð¡ÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ð³Ð¾..."
    )
    
    temp_user_dir = None
    files_to_cleanup: list[Path] = []
    
    try:
        temp_base = Path(config.TEMP_DIR)
        temp_base.mkdir(exist_ok=True)
        temp_user_dir = CleanupManager.create_temp_directory(
            temp_base,
            user_id,
        )
        
        bot = message.bot
        try:
            file: File = await asyncio.wait_for(
                bot.get_file(document.file_id),
                timeout=10.0
            )
        except asyncio.TimeoutError:
            logger.error(f"Timeout getting file info for {document.file_name}")
            await message.answer(
                "âš ï¸ Ð¢Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð¿Ñ€Ð¸ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ð¸ Ñ„Ð°Ð¹Ð»Ð°.\n"
                "ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð¿Ð¾Ð·Ð¶Ðµ Ð¸Ð»Ð¸ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÑŒÑ‚Ðµ Ð´Ñ€ÑƒÐ³Ð¾Ð¹ Ñ„Ð°Ð¹Ð»."
            )
            await processing_msg.delete()
            await state.clear()
            return
        except TelegramNetworkError as e:
            logger.error(f"Network error getting file: {e}")
            await message.answer(
                "âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐµÑ‚Ð¸ Ð¿Ñ€Ð¸ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ð¸.\n"
                "ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚ Ð¸ Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÑÐ½Ð¾Ð²Ð°."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        if not file.file_path:
            await message.answer("âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¿ÑƒÑ‚ÑŒ Ðº Ñ„Ð°Ð¹Ð»Ñƒ.")
            await processing_msg.delete()
            await state.clear()
            return
        
        file_ext = Path(document.file_name or "document").suffix or ".bin"
        temp_file_path = temp_user_dir / f"{uuid.uuid4()}{file_ext}"
        files_to_cleanup.append(temp_file_path)
        
        try:
            await asyncio.wait_for(
                bot.download_file(file.file_path, temp_file_path),
                timeout=30.0
            )
        except asyncio.TimeoutError:
            logger.error(f"Timeout downloading file {document.file_name}")
            await message.answer(
                "âš ï¸ Ð¢Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð¿Ñ€Ð¸ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ð¸ Ñ„Ð°Ð¹Ð»Ð°.\n"
                "ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ñ Ð±Ð¾Ð»ÐµÐµ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ð¼ Ñ„Ð°Ð¹Ð»Ð¾Ð¼."
            )
            await processing_msg.delete()
            await state.clear()
            return
        except TelegramNetworkError as e:
            logger.error(f"Network error downloading file: {e}")
            await message.answer(
                "âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐµÑ‚Ð¸ Ð¿Ñ€Ð¸ ÑÐºÐ°Ñ‡Ð¸Ð²Ð°Ð½Ð¸Ð¸.\n"
                "ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚ Ð¸ Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÑÐ½Ð¾Ð²Ð°."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ñ„Ð°Ð¹Ð»: {temp_file_path.name} ({file_size} bytes)")
        
        await processing_msg.edit_text(
            "ðŸ” ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚...\n"
            "Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐºÑÑ‚Ð°..."
        )
        
        try:
            converter = FileConverter()
            extracted_text = converter.extract_text(temp_file_path, temp_user_dir)
        except ValueError as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð°: {e}")
            await message.answer(
                f"âš ï¸ ÐÐµÑ‚ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¸ ÑÑ‚Ð¾Ð³Ð¾ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð°.\n"
                f"ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‚ÑÑ: PDF, DOCX, TXT, Excel, ZIP, DOC"
            )
            await processing_msg.delete()
            await state.clear()
            return
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ: {type(e).__name__}: {str(e)[:100]}")
            await message.answer(
                f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ð¸ Ñ‚ÐµÐºÑÑ‚Ð°.\n"
                f"ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð´Ñ€ÑƒÐ³Ð¾Ð¹ Ñ„Ð°Ð¹Ð»."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        if not extracted_text or not extracted_text.strip():
            await message.answer(
                "âš ï¸ Ð’ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ðµ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ñ‚ÐµÐºÑÑ‚.\n"
                "ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð´Ñ€ÑƒÐ³Ð¾Ð¹ Ñ„Ð°Ð¹Ð»."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"Document extracted {len(extracted_text)} chars for user {user_id}")
        
        await state.update_data(
            extracted_text=extracted_text,
            original_filename=document.file_name,
            user_id=user_id,
        )
        
        await processing_msg.edit_text(
            "ðŸ” ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚...\n"
            f"ðŸ¤– ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽ Ñ {config.LLM_PROVIDER}..."
        )
        
        analysis_prompt = (
            "ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑÑ‚Ð¾Ñ‚ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ Ð¸ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²ÑŒ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð²Ñ‹Ð²Ð¾Ð´Ñ‹.\n"
            "ÐžÐ¢Ð’Ð•Ð¢ ÐÐ Ð Ð£Ð¡Ð¡ÐšÐžÐœ!"
        )
        
        system_prompt = (
            "Ð¢Ñ‹ Ð²Ð½Ð¸Ð¼Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº. "
            "ÐŸÐ¾Ð¼Ð¾Ð³Ð¸ Ñ€Ð°Ð·Ð±ÐµÑ€Ð°Ñ‚ÑŒÑÑ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°."
        )
        
        try:
            analysis_result = await llm_factory.analyze_document(
                extracted_text,
                analysis_prompt,
                system_prompt=system_prompt,
                use_streaming=False,
                user_id=user_id,
            )
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð›Ð›Ðœ: {type(e).__name__}: {str(e)[:100]}")
            await message.answer(
                f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð°Ð½Ð°Ð»Ð¸Ð·Ð°: {str(e)[:80]}"
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        if not analysis_result:
            await message.answer(
                "âŒ ÐÐ½Ð°Ð»Ð¸Ð· Ð½Ðµ ÑƒÐ´Ð°Ð»ÑÑ. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÑÐ½Ð¾Ð²Ð°."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"Analysis completed ({len(analysis_result)} chars) for user {user_id}")
        
        await processing_msg.delete()
        
        splitter = TextSplitter()
        chunks = splitter.split(analysis_result)
        
        for i, chunk in enumerate(chunks, 1):
            prefix = f"*[Ð§Ð°ÑÑ‚ÑŒ {i}/{len(chunks)}]*\n\n" if len(chunks) > 1 else ""
            try:
                await message.answer(
                    f"{prefix}{chunk}",
                    parse_mode="Markdown",
                )
            except TelegramNetworkError as e:
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐµÑ‚Ð¸ Ð¿Ñ€Ð¸ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐµ: {e}")
                continue
        
        logger.info(
            f"Analysis sent to {user_id} "
            f"({len(chunks)} messages) [{config.LLM_PROVIDER}]"
        )
        await state.clear()
    
    except Exception as e:
        logger.error(f"Error in handle_document: {type(e).__name__}: {str(e)[:100]}")
        try:
            await message.answer(
                f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÑÐ½Ð¾Ð²Ð°."
            )
        except:
            pass
        await state.clear()
    
    finally:
        if files_to_cleanup:
            await CleanupManager.cleanup_files_async(files_to_cleanup)
        if temp_user_dir and temp_user_dir.exists():
            await CleanupManager.cleanup_directory_async(temp_user_dir)


@router.message(
    F.photo,
    ~StateFilter(HomeworkStates, ConversationStates, PromptStates),
)
async def handle_photo(
    message: Message,
    state: FSMContext,
) -> None:
    """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ„Ð¾Ñ‚Ð¾ Ð² ÐžÐ‘Ð©Ð•Ðœ Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ñ OCR.
    
    Args:
        message: User message with photo
        state: FSM state
    """
    current_state = await state.get_state()
    user_id = message.from_user.id
    
    logger.debug(f"[DOCUMENTS DEBUG] User {user_id}: handle_photo called")
    logger.debug(f"[DOCUMENTS DEBUG] User {user_id}: Current state: {current_state}")
    
    if not message.photo:
        await message.answer("âŒ Ð¤Ð¾Ñ‚Ð¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾.")
        return
    
    logger.info(f"documents.handle_photo: User {user_id} uploading photo")
    
    await state.set_state(DocumentAnalysisStates.processing)
    
    processing_msg = await message.answer(
        "ðŸ“‡ ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ Ñ„Ð¾Ñ‚Ð¾...\n"
        "Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ñ‚ÐµÐºÑÑ‚Ð° (OCR)..."
    )
    
    temp_user_dir = None
    files_to_cleanup: list[Path] = []
    
    try:
        temp_base = Path(config.TEMP_DIR)
        temp_base.mkdir(exist_ok=True)
        temp_user_dir = CleanupManager.create_temp_directory(
            temp_base,
            user_id,
        )
        
        extracted_text = await _extract_text_from_photo(message, temp_user_dir, files_to_cleanup)
        
        if not extracted_text or not extracted_text.strip():
            await message.answer(
                "âš ï¸ Ð¢ÐµÐºÑÑ‚ Ð² Ñ„Ð¾Ñ‚Ð¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½.\n"
                "Ð£Ð±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ Ñ‡Ñ‚Ð¾:\n"
                "â€¢ Ð¤Ð¾Ñ‚Ð¾ Ñ‡ÐµÑ‚ÐºÐ¾Ðµ\n"
                "â€¢ Ð¢ÐµÐºÑÑ‚ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾ Ð²Ð¸Ð´ÐµÐ½\n"
                "â€¢ ÐšÐ¾Ð½Ñ‚Ñ€Ð°ÑÑ‚Ð½Ñ‹Ð¹ Ñ„Ð¾Ð½"
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"OCR extracted {len(extracted_text)} chars for user {user_id}")
        
        await state.update_data(
            extracted_text=extracted_text,
            original_filename="photo_document",
            user_id=user_id,
        )
        
        await processing_msg.edit_text(
            "ðŸ“‡ ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ Ñ„Ð¾Ñ‚Ð¾...\n"
            f"ðŸ¤– ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽ Ñ {config.LLM_PROVIDER}..."
        )
        
        analysis_prompt = (
            "ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑÑ‚Ð¾Ñ‚ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ Ð¸ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²ÑŒ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð²Ñ‹Ð²Ð¾Ð´Ñ‹.\n"
            "ÐžÐ¢Ð’Ð•Ð¢ ÐÐ Ð Ð£Ð¡Ð¡ÐšÐžÐœ!"
        )
        
        system_prompt = (
            "Ð¢Ñ‹ Ð²Ð½Ð¸Ð¼Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº. "
            "ÐŸÐ¾Ð¼Ð¾Ð³Ð¸ Ñ€Ð°Ð·Ð±ÐµÑ€Ð°Ñ‚ÑŒÑÑ Ð² Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð°Ñ… Ñ„Ð¾Ñ‚Ð¾."
        )
        
        try:
            analysis_result = await llm_factory.analyze_document(
                extracted_text,
                analysis_prompt,
                system_prompt=system_prompt,
                use_streaming=False,
                user_id=user_id,
            )
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð›Ð›Ðœ: {type(e).__name__}: {str(e)[:100]}")
            await message.answer(
                f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð°Ð½Ð°Ð»Ð¸Ð·Ð°: {str(e)[:80]}"
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        if not analysis_result:
            await message.answer(
                "âŒ ÐÐ½Ð°Ð»Ð¸Ð· Ð½Ðµ ÑƒÐ´Ð°Ð»ÑÑ. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÑÐ½Ð¾Ð²Ð°."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"Analysis completed ({len(analysis_result)} chars) for user {user_id}")
        
        await processing_msg.delete()
        
        splitter = TextSplitter()
        chunks = splitter.split(analysis_result)
        
        for i, chunk in enumerate(chunks, 1):
            prefix = f"*[Ð§Ð°ÑÑ‚ÑŒ {i}/{len(chunks)}]*\n\n" if len(chunks) > 1 else ""
            try:
                await message.answer(
                    f"{prefix}{chunk}",
                    parse_mode="Markdown",
                )
            except TelegramNetworkError as e:
                logger.error(f"Network error sending: {e}")
                continue
        
        logger.info(
            f"Photo analysis sent to {user_id} "
            f"({len(chunks)} messages) [{config.LLM_PROVIDER}]"
        )
        await state.clear()
    
    except Exception as e:
        logger.error(f"Error in handle_photo: {type(e).__name__}: {str(e)[:100]}")
        try:
            await message.answer(
                f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ°. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÑÐ½Ð¾Ð²Ð°."
            )
        except:
            pass
        await state.clear()
    
    finally:
        if files_to_cleanup:
            await CleanupManager.cleanup_files_async(files_to_cleanup)
        if temp_user_dir and temp_user_dir.exists():
            await CleanupManager.cleanup_directory_async(temp_user_dir)


async def _extract_text_from_photo(
    message: Message,
    temp_dir: Path,
    cleanup_list: list[Path],
) -> str:
    """Extract text from photo using OCR.space API.
    
    Args:
        message: Message with photo
        temp_dir: Temporary directory
        cleanup_list: List to add files for cleanup
        
    Returns:
        Extracted text from photo
    """
    try:
        import httpx
        
        photo = message.photo[-1]
        file_info = await message.bot.get_file(photo.file_id)
        
        temp_file = temp_dir / f"photo_{photo.file_unique_id}.jpg"
        await message.bot.download_file(file_info.file_path, temp_file)
        cleanup_list.append(temp_file)
        
        with open(temp_file, "rb") as f:
            photo_bytes = f.read()
        
        photo_base64 = base64.b64encode(photo_bytes).decode("utf-8")
        
        async with httpx.AsyncClient() as client:
            response = await asyncio.wait_for(
                client.post(
                    "https://api.ocr.space/parse/image",
                    data={
                        "apikey": config.OCR_SPACE_API_KEY,
                        "base64Image": f"data:image/jpeg;base64,{photo_base64}",
                        "language": "rus",
                        "isOverlayRequired": False,
                        "detectOrientation": True,
                        "scale": True,
                        "OCREngine": 2,
                    },
                ),
                timeout=30.0,
            )
            
            if response.status_code != 200:
                logger.error(f"OCR error: {response.status_code}")
                return ""
            
            result = response.json()
            
            if result.get("IsErroredOnProcessing"):
                error_msg = result.get("ErrorMessage", "Unknown")
                logger.error(f"OCR error: {error_msg}")
                return ""
            
            parsed_results = result.get("ParsedResults", [])
            if not parsed_results:
                logger.warning("OCR: No text detected")
                return ""
            
            text = parsed_results[0].get("ParsedText", "")
            logger.info(f"OCR: Extracted {len(text)} chars")
            return text.strip()
    
    except asyncio.TimeoutError:
        logger.error("OCR: Timeout")
        return ""
    except Exception as e:
        logger.error(f"OCR error: {type(e).__name__}: {str(e)}")
        return ""

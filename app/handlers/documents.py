"""–î–æ–∫—É–º–µ–Ω—Ç —Ö–∞–Ω–¥–ª–µ—Ä—ã –¥–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤.

Handles file uploads, processing, and analysis responses.
Supports multiple LLM providers with fallback.

UPDATED 2025-12-28 23:10:
- REPLACED: OCR.space API ‚Üí LOCAL Tesseract/EasyOCR (no SSL issues!)
- ADDED: Unified OCR Service across all modes
- FIXED: JPG recognition now works in all modes
- IMPROVED: Quality assessment for OCR results

UPDATED 2025-12-25 14:45:
- –£–î–ê–õ–ï–ù—ã –¥—É–±–ª–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –∏ –ø—Ä–æ–º–ø—Ç–æ–≤
- –í—Å–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ø–µ—Ä—å –≤ replicate_client.py
"""

import logging
import tempfile
import uuid
import asyncio
from pathlib import Path

from aiogram import Router, F
from aiogram.types import Message, Document, File
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State
from aiogram.filters import StateFilter
from aiogram.enums import ContentType
from aiogram.exceptions import TelegramNetworkError

from app.config import get_settings
from app.states.analysis import DocumentAnalysisStates
from app.states.homework import HomeworkStates
from app.states.conversation import ConversationStates
from app.states.prompts import PromptStates
from app.services.file_processing.converter import FileConverter
from app.services.llm.llm_factory import LLMFactory
from app.services.ocr import OCRService, OCRQualityLevel
from app.utils.text_splitter import TextSplitter
from app.utils.cleanup import CleanupManager

logger = logging.getLogger(__name__)

router = Router()
config = get_settings()
ocr_service = OCRService()

llm_factory = LLMFactory(
    primary_provider=config.LLM_PROVIDER,
    openai_api_key=config.OPENAI_API_KEY or None,
    openai_model=config.OPENAI_MODEL,
    replicate_api_token=config.REPLICATE_API_TOKEN or None,
    replicate_model=config.REPLICATE_MODEL,
)


@router.message(
    F.document,
    ~StateFilter(HomeworkStates, ConversationStates, PromptStates),
)
async def handle_document(
    message: Message,
    state: FSMContext,
) -> None:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –û–ë–©–ï–ú —Ä–µ–∂–∏–º–µ.
    
    Args:
        message: User message with document
        state: FSM state
    """
    current_state = await state.get_state()
    user_id = message.from_user.id
    
    logger.debug(f"[DOCUMENTS DEBUG] User {user_id}: handle_document called")
    logger.debug(f"[DOCUMENTS DEBUG] User {user_id}: Current state: {current_state}")
    
    if not message.document:
        await message.answer("‚ùå –î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω.")
        return
    
    document: Document = message.document
    file_size = document.file_size or 0
    
    logger.info(f"documents.handle_document: User {user_id} uploading {document.file_name}")
    
    if file_size > config.MAX_FILE_SIZE:
        max_size_mb = config.MAX_FILE_SIZE / (1024 * 1024)
        await message.answer(
            f"‚ö†Ô∏è –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {file_size / (1024 * 1024):.1f} MB\n"
            f"–ú–∞–∫—Å–∏–º—É–º: {max_size_mb:.1f} MB",
        )
        return
    
    await state.set_state(DocumentAnalysisStates.processing)
    
    processing_msg = await message.answer(
        "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –¥–æ–∫—É–º–µ–Ω—Ç...\n"
        "–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ..."
    )
    
    temp_user_dir = None
    files_to_cleanup: list[Path] = []
    
    try:
        temp_base = Path(config.TEMP_DIR)
        temp_base.mkdir(exist_ok=True)
        temp_user_dir = CleanupManager.create_temp_directory(
            temp_base,
            user_id,
        )
        
        bot = message.bot
        try:
            file: File = await asyncio.wait_for(
                bot.get_file(document.file_id),
                timeout=10.0
            )
        except asyncio.TimeoutError:
            logger.error(f"Timeout getting file info for {document.file_name}")
            await message.answer(
                "‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞.\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –¥—Ä—É–≥–æ–π —Ñ–∞–π–ª."
            )
            await processing_msg.delete()
            await state.clear()
            return
        except TelegramNetworkError as e:
            logger.error(f"Network error getting file: {e}")
            await message.answer(
                "‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏.\n"
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        if not file.file_path:
            await message.answer("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É.")
            await processing_msg.delete()
            await state.clear()
            return
        
        file_ext = Path(document.file_name or "document").suffix or ".bin"
        temp_file_path = temp_user_dir / f"{uuid.uuid4()}{file_ext}"
        files_to_cleanup.append(temp_file_path)
        
        try:
            await asyncio.wait_for(
                bot.download_file(file.file_path, temp_file_path),
                timeout=30.0
            )
        except asyncio.TimeoutError:
            logger.error(f"Timeout downloading file {document.file_name}")
            await message.answer(
                "‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞.\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å –±–æ–ª–µ–µ –º–∞–ª—ã–º —Ñ–∞–π–ª–æ–º."
            )
            await processing_msg.delete()
            await state.clear()
            return
        except TelegramNetworkError as e:
            logger.error(f"Network error downloading file: {e}")
            await message.answer(
                "‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏.\n"
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {temp_file_path.name} ({file_size} bytes)")
        
        await processing_msg.edit_text(
            "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –¥–æ–∫—É–º–µ–Ω—Ç...\n"
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ..."
        )
        
        try:
            converter = FileConverter()
            extracted_text = converter.extract_text(temp_file_path, temp_user_dir)
        except ValueError as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞: {e}")
            await message.answer(
                f"‚ö†Ô∏è –ù–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —ç—Ç–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞.\n"
                f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: PDF, DOCX, TXT, Excel, ZIP, DOC"
            )
            await processing_msg.delete()
            await state.clear()
            return
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {type(e).__name__}: {str(e)[:100]}")
            await message.answer(
                f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞.\n"
                f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ñ–∞–π–ª."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        if not extracted_text or not extracted_text.strip():
            await message.answer(
                "‚ö†Ô∏è –í –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç.\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ñ–∞–π–ª."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"Document extracted {len(extracted_text)} chars for user {user_id}")
        
        await state.update_data(
            extracted_text=extracted_text,
            original_filename=document.file_name,
            user_id=user_id,
        )
        
        await processing_msg.edit_text(
            "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –¥–æ–∫—É–º–µ–Ω—Ç...\n"
            f"ü§ñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å {config.LLM_PROVIDER}..."
        )
        
        analysis_prompt = (
            "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –∫–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã.\n"
            "–û–¢–í–ï–¢ –ù–ê –†–£–°–°–ö–û–ú!"
        )
        
        system_prompt = (
            "–¢—ã –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. "
            "–ü–æ–º–æ–≥–∏ —Ä–∞–∑–±–µ—Ä–∞—Ç—å—Å—è –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞."
        )
        
        try:
            analysis_result = await llm_factory.analyze_document(
                extracted_text,
                analysis_prompt,
                system_prompt=system_prompt,
                use_streaming=False,
                user_id=user_id,
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –õ–õ–ú: {type(e).__name__}: {str(e)[:100]}")
            await message.answer(
                f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)[:80]}"
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        if not analysis_result:
            await message.answer(
                "‚ùå –ê–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"Analysis completed ({len(analysis_result)} chars) for user {user_id}")
        
        await processing_msg.delete()
        
        splitter = TextSplitter()
        chunks = splitter.split(analysis_result)
        
        for i, chunk in enumerate(chunks, 1):
            prefix = f"*[–ß–∞—Å—Ç—å {i}/{len(chunks)}]*\n\n" if len(chunks) > 1 else ""
            try:
                await message.answer(
                    f"{prefix}{chunk}",
                    parse_mode="Markdown",
                )
            except TelegramNetworkError as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ: {e}")
                continue
        
        logger.info(
            f"Analysis sent to {user_id} "
            f"({len(chunks)} messages) [{config.LLM_PROVIDER}]"
        )
        await state.clear()
    
    except Exception as e:
        logger.error(f"Error in handle_document: {type(e).__name__}: {str(e)[:100]}")
        try:
            await message.answer(
                f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            )
        except:
            pass
        await state.clear()
    
    finally:
        if files_to_cleanup:
            await CleanupManager.cleanup_files_async(files_to_cleanup)
        if temp_user_dir and temp_user_dir.exists():
            await CleanupManager.cleanup_directory_async(temp_user_dir)


@router.message(
    F.photo,
    ~StateFilter(HomeworkStates, ConversationStates, PromptStates),
)
async def handle_photo(
    message: Message,
    state: FSMContext,
) -> None:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ –≤ –û–ë–©–ï–ú —Ä–µ–∂–∏–º–µ —Å LOCAL OCR.
    
    Args:
        message: User message with photo
        state: FSM state
    """
    current_state = await state.get_state()
    user_id = message.from_user.id
    
    logger.debug(f"[DOCUMENTS DEBUG] User {user_id}: handle_photo called")
    logger.debug(f"[DOCUMENTS DEBUG] User {user_id}: Current state: {current_state}")
    
    if not message.photo:
        await message.answer("‚ùå –§–æ—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return
    
    logger.info(f"documents.handle_photo: User {user_id} uploading photo")
    
    await state.set_state(DocumentAnalysisStates.processing)
    
    processing_msg = await message.answer(
        "üìó –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–æ—Ç–æ...\n"
        "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (OCR)..."
    )
    
    temp_user_dir = None
    files_to_cleanup: list[Path] = []
    
    try:
        temp_base = Path(config.TEMP_DIR)
        temp_base.mkdir(exist_ok=True)
        temp_user_dir = CleanupManager.create_temp_directory(
            temp_base,
            user_id,
        )
        
        # Download photo
        photo = message.photo[-1]
        file_info = await message.bot.get_file(photo.file_id)
        temp_file = temp_user_dir / f"photo_{photo.file_unique_id}.jpg"
        await message.bot.download_file(file_info.file_path, temp_file)
        files_to_cleanup.append(temp_file)
        
        logger.info(f"Downloaded photo: {temp_file}")
        
        # Extract text using unified OCR service
        extracted_text, quality = await ocr_service.extract_from_file(temp_file, user_id)
        
        logger.info(
            f"[OCR QUALITY] User {user_id}: {quality.value} | "
            f"{len(extracted_text)} chars, {len(extracted_text.split())} words"
        )
        
        if not extracted_text or not extracted_text.strip():
            await message.answer(
                "‚ö†Ô∏è –¢–µ–∫—Å—Ç –≤ —Ñ–æ—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω.\n"
                "–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ:\n"
                "‚Ä¢ –§–æ—Ç–æ —á–µ—Ç–∫–æ–µ\n"
                "‚Ä¢ –¢–µ–∫—Å—Ç —Ö–æ—Ä–æ—à–æ –≤–∏–¥–µ–Ω\n"
                "‚Ä¢ –ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–π —Ñ–æ–Ω\n\n"
                "*‚ö†Ô∏è –í–ê–ñ–ù–û:* –†—É–∫–æ–ø–∏—Å–Ω—ã–π —Ç–µ–∫—Å—Ç –º–æ–∂–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å—Å—è –Ω–µ–≤–µ—Ä–Ω–æ. "
                "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ PDF –∏–ª–∏ —á–µ—Ç–∫–∏–µ —Ñ–æ—Ç–æ –ø–µ—á–∞—Ç–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        if quality == OCRQualityLevel.POOR:
            await message.answer(
                "‚ö†Ô∏è –ö–ê–ß–ï–°–¢–í–û OCR: –ù–∏–∑–∫–æ–µ\n"
                "–ú–æ–∂–Ω–æ —Å–ª–æ–∂–Ω–∞—è —Ä—É–∫–æ–ø–∏—Å—å –∏–ª–∏ –Ω–∏–∑–∫–∞—è —Ç–µ–∫—Å—Ç—É—Ä–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"OCR extracted {len(extracted_text)} chars for user {user_id}")
        
        await state.update_data(
            extracted_text=extracted_text,
            original_filename="photo_document",
            user_id=user_id,
        )
        
        await processing_msg.edit_text(
            "üìó –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–æ—Ç–æ...\n"
            f"ü§ñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å {config.LLM_PROVIDER}..."
        )
        
        analysis_prompt = (
            "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –∫–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã.\n"
            "–û–¢–í–ï–¢ –ù–ê –†–£–°–°–ö–û–ú!"
        )
        
        system_prompt = (
            "–¢—ã –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. "
            "–ü–æ–º–æ–≥–∏ —Ä–∞–∑–±–µ—Ä–∞—Ç—å—Å—è –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö —Ñ–æ—Ç–æ."
        )
        
        try:
            analysis_result = await llm_factory.analyze_document(
                extracted_text,
                analysis_prompt,
                system_prompt=system_prompt,
                use_streaming=False,
                user_id=user_id,
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –õ–õ–ú: {type(e).__name__}: {str(e)[:100]}")
            await message.answer(
                f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)[:80]}"
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        if not analysis_result:
            await message.answer(
                "‚ùå –ê–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"Analysis completed ({len(analysis_result)} chars) for user {user_id}")
        
        await processing_msg.delete()
        
        splitter = TextSplitter()
        chunks = splitter.split(analysis_result)
        
        for i, chunk in enumerate(chunks, 1):
            prefix = f"*[–ß–∞—Å—Ç—å {i}/{len(chunks)}]*\n\n" if len(chunks) > 1 else ""
            try:
                await message.answer(
                    f"{prefix}{chunk}",
                    parse_mode="Markdown",
                )
            except TelegramNetworkError as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ: {e}")
                continue
        
        logger.info(
            f"Photo analysis sent to {user_id} "
            f"({len(chunks)} messages) [{config.LLM_PROVIDER}]"
        )
        await state.clear()
    
    except Exception as e:
        logger.error(f"Error in handle_photo: {type(e).__name__}: {str(e)[:100]}")
        try:
            await message.answer(
                f"‚ùå –û—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            )
        except:
            pass
        await state.clear()
    
    finally:
        if files_to_cleanup:
            await CleanupManager.cleanup_files_async(files_to_cleanup)
        if temp_user_dir and temp_user_dir.exists():
            await CleanupManager.cleanup_directory_async(temp_user_dir)

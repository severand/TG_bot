"""–î–æ–∫—É–º–µ–Ω—Ç —Ö–∞–Ω–¥–ª–µ—Ä—ã –¥–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤.

–§–∏–∫—Å 2025-12-25 12:27:
- –õ–æ–≥–∏—Ä—É–µ–º —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–∏ –∏ OCR
- –õ–æ–≥–∏—Ä—É–µ–º system_prompt (–æ–±—â–∏–π –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
- –õ–æ–≥–∏—Ä—É–µ–º user_prompt –∏ –µ–≥–æ —Ä–∞–∑–º–µ—Ä

Handles file uploads, processing, and analysis responses.
Supports multiple LLM providers with fallback.
"""

import logging
import tempfile
import uuid
import base64
import asyncio
from pathlib import Path

from aiogram import Router, F
from aiogram.types import Message, Document, File
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State
from aiogram.filters import StateFilter
from aiogram.enums import ContentType
from aiogram.exceptions import TelegramNetworkError

from app.config import get_settings
from app.states.analysis import DocumentAnalysisStates
from app.states.homework import HomeworkStates
from app.states.conversation import ConversationStates
from app.states.prompts import PromptStates
from app.services.file_processing.converter import FileConverter
from app.services.llm.llm_factory import LLMFactory
from app.utils.text_splitter import TextSplitter
from app.utils.cleanup import CleanupManager

logger = logging.getLogger(__name__)

router = Router()
config = get_settings()

llm_factory = LLMFactory(
    primary_provider=config.LLM_PROVIDER,
    openai_api_key=config.OPENAI_API_KEY or None,
    openai_model=config.OPENAI_MODEL,
    replicate_api_token=config.REPLICATE_API_TOKEN or None,
    replicate_model=config.REPLICATE_MODEL,
)


@router.message(
    F.document,
    ~StateFilter(HomeworkStates, ConversationStates, PromptStates),
)
async def handle_document(
    message: Message,
    state: FSMContext,
) -> None:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –û–ë–©–ï–ú —Ä–µ–∂–∏–º–µ.
    
    Args:
        message: User message with document
        state: FSM state
    """
    current_state = await state.get_state()
    user_id = message.from_user.id
    
    logger.debug(f"[DOCUMENTS DEBUG] User {user_id}: handle_document called")
    logger.debug(f"[DOCUMENTS DEBUG] User {user_id}: Current state: {current_state}")
    
    if not message.document:
        await message.answer("‚ùå –î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω.")
        return
    
    document: Document = message.document
    file_size = document.file_size or 0
    
    logger.info(f"documents.handle_document: User {user_id} uploading {document.file_name}")
    
    if file_size > config.MAX_FILE_SIZE:
        max_size_mb = config.MAX_FILE_SIZE / (1024 * 1024)
        await message.answer(
            f"‚ö†Ô∏è –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {file_size / (1024 * 1024):.1f} MB\n"
            f"–ú–∞–∫—Å–∏–º—É–º: {max_size_mb:.1f} MB",
        )
        return
    
    await state.set_state(DocumentAnalysisStates.processing)
    
    processing_msg = await message.answer(
        "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –¥–æ–∫—É–º–µ–Ω—Ç...\n"
        "–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ..."
    )
    
    temp_user_dir = None
    files_to_cleanup: list[Path] = []
    
    try:
        temp_base = Path(config.TEMP_DIR)
        temp_base.mkdir(exist_ok=True)
        temp_user_dir = CleanupManager.create_temp_directory(
            temp_base,
            user_id,
        )
        
        bot = message.bot
        try:
            file: File = await asyncio.wait_for(
                bot.get_file(document.file_id),
                timeout=10.0
            )
        except asyncio.TimeoutError:
            logger.error(f"Timeout getting file info for {document.file_name}")
            await message.answer(
                "‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞.\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –¥—Ä—É–≥–æ–π —Ñ–∞–π–ª."
            )
            await processing_msg.delete()
            await state.clear()
            return
        except TelegramNetworkError as e:
            logger.error(f"Network error getting file: {e}")
            await message.answer(
                "‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏.\n"
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        if not file.file_path:
            await message.answer("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É.")
            await processing_msg.delete()
            await state.clear()
            return
        
        file_ext = Path(document.file_name or "document").suffix or ".bin"
        temp_file_path = temp_user_dir / f"{uuid.uuid4()}{file_ext}"
        files_to_cleanup.append(temp_file_path)
        
        try:
            await asyncio.wait_for(
                bot.download_file(file.file_path, temp_file_path),
                timeout=30.0
            )
        except asyncio.TimeoutError:
            logger.error(f"Timeout downloading file {document.file_name}")
            await message.answer(
                "‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞.\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å –±–æ–ª–µ–µ –º–∞–ª–µ–Ω—å–∫–∏–º —Ñ–∞–π–ª–æ–º."
            )
            await processing_msg.delete()
            await state.clear()
            return
        except TelegramNetworkError as e:
            logger.error(f"Network error downloading file: {e}")
            await message.answer(
                "‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏.\n"
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {temp_file_path.name} ({file_size} bytes)")
        
        await processing_msg.edit_text(
            "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –¥–æ–∫—É–º–µ–Ω—Ç...\n"
            "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞..."
        )
        
        try:
            converter = FileConverter()
            extracted_text = converter.extract_text(temp_file_path, temp_user_dir)
        except ValueError as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞: {e}")
            await message.answer(
                f"‚ö†Ô∏è –ù–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —ç—Ç–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞.\n"
                f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è: PDF, DOCX, TXT, Excel, ZIP, DOC"
            )
            await processing_msg.delete()
            await state.clear()
            return
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {type(e).__name__}: {str(e)[:100]}")
            await message.answer(
                f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞.\n"
                f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ñ–∞–π–ª."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        if not extracted_text or not extracted_text.strip():
            await message.answer(
                "‚ö†Ô∏è –í –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç.\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ñ–∞–π–ª."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"[DOCUMENTS TEXT] User {user_id}: –≠–∫—Å—Ç—Ä–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(extracted_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        logger.info(f"[DOCUMENTS TEXT RAW] User {user_id} ({len(extracted_text)} chars):\n{extracted_text[:500]}..." if len(extracted_text) > 500 else f"[DOCUMENTS TEXT RAW] User {user_id}:\n{extracted_text}")
        
        await state.update_data(
            extracted_text=extracted_text,
            original_filename=document.file_name,
            user_id=user_id,
        )
        
        await processing_msg.edit_text(
            "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –¥–æ–∫—É–º–µ–Ω—Ç...\n"
            f"ü§ñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å {config.LLM_PROVIDER}..."
        )
        
        analysis_prompt = (
            "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –∫–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã.\n"
            "–û–¢–í–ï–¢ –ù–ê –†–£–°–°–ö–û–ú!"
        )
        
        # LOG: –û–±—â–∏–π system prompt –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        system_prompt = (
            "–¢—ã –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. "
            "–ü–æ–º–æ–≥–∏ —Ä–∞–∑–±–µ—Ä–∞—Ç—å—Å—è –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞."
        )
        logger.info(f"[DOCUMENTS SYSTEM PROMPT] User {user_id}:\n{system_prompt}")
        
        # LOG: User prompt
        logger.info(f"[DOCUMENTS USER PROMPT] User {user_id}:\n{analysis_prompt}")
        
        try:
            analysis_result = await llm_factory.analyze_document(
                extracted_text,
                analysis_prompt,
                use_streaming=False,
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –õ–õ–ú: {type(e).__name__}: {str(e)[:100]}")
            await message.answer(
                f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)[:80]}"
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        if not analysis_result:
            await message.answer(
                "‚ùå –ê–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –æ–∫–æ–Ω—á–µ–Ω ({len(analysis_result)} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        await processing_msg.delete()
        
        splitter = TextSplitter()
        chunks = splitter.split(analysis_result)
        
        for i, chunk in enumerate(chunks, 1):
            prefix = f"*[–ß–∞—Å—Ç—å {i}/{len(chunks)}]*\n\n" if len(chunks) > 1 else ""
            try:
                await message.answer(
                    f"{prefix}{chunk}",
                    parse_mode="Markdown",
                )
            except TelegramNetworkError as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ: {e}")
                continue
        
        logger.info(
            f"–ê–Ω–∞–ª–∏–∑ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω {user_id} "
            f"({len(chunks)} —Å–æ–æ–±—â–µ–Ω–∏–π) [{config.LLM_PROVIDER}]"
        )
        await state.clear()
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞–±–æ—Ç—ã: {type(e).__name__}: {str(e)[:100]}")
        try:
            await message.answer(
                f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            )
        except:
            pass
        await state.clear()
    
    finally:
        if files_to_cleanup:
            await CleanupManager.cleanup_files_async(files_to_cleanup)
        if temp_user_dir and temp_user_dir.exists():
            await CleanupManager.cleanup_directory_async(temp_user_dir)


@router.message(
    F.photo,
    ~StateFilter(HomeworkStates, ConversationStates, PromptStates),
)
async def handle_photo(
    message: Message,
    state: FSMContext,
) -> None:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ –≤ –û–ë–©–ï–ú —Ä–µ–∂–∏–º–µ —Å OCR.
    
    Args:
        message: User message with photo
        state: FSM state
    """
    current_state = await state.get_state()
    user_id = message.from_user.id
    
    logger.debug(f"[DOCUMENTS DEBUG] User {user_id}: handle_photo called")
    logger.debug(f"[DOCUMENTS DEBUG] User {user_id}: Current state: {current_state}")
    
    if not message.photo:
        await message.answer("‚ùå –§–æ—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return
    
    logger.info(f"documents.handle_photo: User {user_id} uploading photo")
    
    await state.set_state(DocumentAnalysisStates.processing)
    
    processing_msg = await message.answer(
        "üìá –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–æ—Ç–æ...\n"
        "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (OCR)..."
    )
    
    temp_user_dir = None
    files_to_cleanup: list[Path] = []
    
    try:
        temp_base = Path(config.TEMP_DIR)
        temp_base.mkdir(exist_ok=True)
        temp_user_dir = CleanupManager.create_temp_directory(
            temp_base,
            user_id,
        )
        
        extracted_text = await _extract_text_from_photo(message, temp_user_dir, files_to_cleanup)
        
        if not extracted_text or not extracted_text.strip():
            await message.answer(
                "‚ö†Ô∏è –¢–µ–∫—Å—Ç –≤ —Ñ–æ—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω.\n"
                "–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ:\n"
                "‚Ä¢ –§–æ—Ç–æ —á–µ—Ç–∫–æ–µ\n"
                "‚Ä¢ –¢–µ–∫—Å—Ç —Ö–æ—Ä–æ—à–æ –≤–∏–¥–µ–Ω\n"
                "‚Ä¢ –ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–π —Ñ–æ–Ω"
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"[DOCUMENTS TEXT] User {user_id}: OCR –≠–∫—Å—Ç—Ä–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(extracted_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        logger.info(f"[DOCUMENTS TEXT RAW] User {user_id} ({len(extracted_text)} chars):\n{extracted_text}")
        
        await state.update_data(
            extracted_text=extracted_text,
            original_filename="photo_document",
            user_id=user_id,
        )
        
        await processing_msg.edit_text(
            "üìá –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–æ—Ç–æ...\n"
            f"ü§ñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å {config.LLM_PROVIDER}..."
        )
        
        analysis_prompt = (
            "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –∫–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã.\n"
            "–û–¢–í–ï–¢ –ù–ê –†–£–°–°–ö–û–ú!"
        )
        
        system_prompt = (
            "–¢—ã –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. "
            "–ü–æ–º–æ–≥–∏ —Ä–∞–∑–±–µ—Ä–∞—Ç—å—Å—è –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö —Ñ–æ—Ç–æ."
        )
        logger.info(f"[DOCUMENTS SYSTEM PROMPT] User {user_id}:\n{system_prompt}")
        logger.info(f"[DOCUMENTS USER PROMPT] User {user_id}:\n{analysis_prompt}")
        
        try:
            analysis_result = await llm_factory.analyze_document(
                extracted_text,
                analysis_prompt,
                use_streaming=False,
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –õ–õ–ú: {type(e).__name__}: {str(e)[:100]}")
            await message.answer(
                f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)[:80]}"
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        if not analysis_result:
            await message.answer(
                "‚ùå –ê–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            )
            await processing_msg.delete()
            await state.clear()
            return
        
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –æ–∫–æ–Ω—á–µ–Ω ({len(analysis_result)} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        await processing_msg.delete()
        
        splitter = TextSplitter()
        chunks = splitter.split(analysis_result)
        
        for i, chunk in enumerate(chunks, 1):
            prefix = f"*[–ß–∞—Å—Ç—å {i}/{len(chunks)}]*\n\n" if len(chunks) > 1 else ""
            try:
                await message.answer(
                    f"{prefix}{chunk}",
                    parse_mode="Markdown",
                )
            except TelegramNetworkError as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}")
                continue
        
        logger.info(
            f"–û—Å–∞–Ω–∞–ª–∏–∑ —Ñ–æ—Ç–æ {user_id} "
            f"({len(chunks)} —Å–æ–æ–±—â–µ–Ω–∏–π) [{config.LLM_PROVIDER}]"
        )
        await state.clear()
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞: {type(e).__name__}: {str(e)[:100]}")
        try:
            await message.answer(
                f"‚ùå –û—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞."
            )
        except:
            pass
        await state.clear()
    
    finally:
        if files_to_cleanup:
            await CleanupManager.cleanup_files_async(files_to_cleanup)
        if temp_user_dir and temp_user_dir.exists():
            await CleanupManager.cleanup_directory_async(temp_user_dir)


async def _extract_text_from_photo(
    message: Message,
    temp_dir: Path,
    cleanup_list: list[Path],
) -> str:
    """Extract text from photo using OCR.space API.
    
    Args:
        message: Message with photo
        temp_dir: Temporary directory
        cleanup_list: List to add files for cleanup
        
    Returns:
        Extracted text from photo
    """
    try:
        import httpx
        
        photo = message.photo[-1]
        file_info = await message.bot.get_file(photo.file_id)
        
        temp_file = temp_dir / f"photo_{photo.file_unique_id}.jpg"
        await message.bot.download_file(file_info.file_path, temp_file)
        cleanup_list.append(temp_file)
        
        with open(temp_file, "rb") as f:
            photo_bytes = f.read()
        
        photo_base64 = base64.b64encode(photo_bytes).decode("utf-8")
        
        async with httpx.AsyncClient() as client:
            response = await asyncio.wait_for(
                client.post(
                    "https://api.ocr.space/parse/image",
                    data={
                        "apikey": config.OCR_SPACE_API_KEY,
                        "base64Image": f"data:image/jpeg;base64,{photo_base64}",
                        "language": "rus",
                        "isOverlayRequired": False,
                        "detectOrientation": True,
                        "scale": True,
                        "OCREngine": 2,
                    },
                ),
                timeout=30.0,
            )
            
            if response.status_code != 200:
                logger.error(f"–û–ß–† –æ—à–∏–±–∫–∞: {response.status_code}")
                return ""
            
            result = response.json()
            
            if result.get("IsErroredOnProcessing"):
                error_msg = result.get("ErrorMessage", "Unknown")
                logger.error(f"–û–ß–† –æ—à–∏–±–∫–∞: {error_msg}")
                return ""
            
            parsed_results = result.get("ParsedResults", [])
            if not parsed_results:
                logger.warning("–û–ß–†: –¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return ""
            
            text = parsed_results[0].get("ParsedText", "")
            logger.info(f"–û–ß–†: –í—ã–¥–µ–ª–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            return text.strip()
    
    except asyncio.TimeoutError:
        logger.error("–û–ß–†: –¢–∞–π–º–∞—É—Ç")
        return ""
    except Exception as e:
        logger.error(f"–û–ß–† –æ—à–∏–±–∫–∞: {type(e).__name__}: {str(e)}")
        return ""

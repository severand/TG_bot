"""Conversation mode handlers for interactive document analysis.

UPDATED 2025-12-25 14:48:
- Added user_id parameter to analyze_document calls
- All logging now includes user context

Fixes 2025-12-25 11:27:
- –ê–†–•–ò–¢–ï–ö–¢–£–†–ù–ê–Ø –û–ü–¢–ò–ú–∏–∑–∞—Ü–∏—è: –≠–ö–°–ü–õ–ò–¶–ò–¢–ù–´–ï state filters –≤ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–∞—Ö
- –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –¢–û–õ–¨–ö–û –∫–æ–≥–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –í ConversationStates.ready
- –í –¥—Ä—É–≥–∏—Ö —Ä–µ–∂–∏–º–∞—Ö (homework, prompts) –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞–º–∏
- –ù–ò–ö–ê–ö–û–ì–û –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏ –≤–æ–≤–ª–µ

–§–∏–∫—Å—ã 2025-12-21 14:16:
- –£–ë–†–ê–ù–û –º–∏–≥–∞—é—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–æ–º —Ç–µ–∫—Å—Ç–∞
- –¢–µ–ø–µ—Ä—å: –û–±—Ä–∞–±–æ—Ç–∫–∞ ‚Üí –ê–Ω–∞–ª–∏–∑ (–±–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –ø–æ–∫–∞–∑–æ–≤)

Handles document analysis and user prompts for interactive conversation.
"""

import logging
import uuid
from pathlib import Path

from aiogram import Router, F
from aiogram.filters import Command
from aiogram.types import Message, Document, File, CallbackQuery, InlineKeyboardButton, InlineKeyboardMarkup
from aiogram.fsm.context import FSMContext
from aiogram.utils.keyboard import InlineKeyboardBuilder

from app.config import get_settings
from app.states.conversation import ConversationStates
from app.services.file_processing.converter import FileConverter
from app.services.llm.llm_factory import LLMFactory
from app.services.prompts.prompt_manager import PromptManager
from app.utils.text_splitter import TextSplitter
from app.utils.cleanup import CleanupManager

logger = logging.getLogger(__name__)

router = Router()
config = get_settings()
prompt_manager = PromptManager()
llm_factory = LLMFactory(
    primary_provider=config.LLM_PROVIDER,
    openai_api_key=config.OPENAI_API_KEY or None,
    openai_model=config.OPENAI_MODEL,
    replicate_api_token=config.REPLICATE_API_TOKEN or None,
    replicate_model=config.REPLICATE_MODEL,
)


def _get_prompts_keyboard(user_id: int) -> InlineKeyboardMarkup:
    """Get keyboard with ONLY document analysis prompts - 2 buttons per row.
    
    –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º get_prompt_by_category() –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è
    –¢–û–õ–¨–ö–û –ø—Ä–æ–º–ø—Ç–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ "document_analysis", –∞ –ù–ï –≤—Å–µ—Ö –ø—Ä–æ–º–ø—Ç–æ–≤.
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    prompt_manager.load_user_prompts(user_id)
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–ª—É—á–∞–µ–º –¢–û–õ–¨–ö–û –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
    prompts = prompt_manager.get_prompt_by_category(user_id, "document_analysis")
    
    logger.debug(f"User {user_id}: Loading {len(prompts)} DOCUMENT ANALYSIS prompts")
    
    builder = InlineKeyboardBuilder()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–Ω–æ–ø–∫–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
    for name in sorted(prompts.keys()):
        prompt = prompts[name]
        button_text = f"{prompt.description[:40]}"
        builder.button(
            text=button_text,
            callback_data=f"analyze_select_prompt_{name}"
        )
    
    # –ö–Ω–æ–ø–∫–∞ –æ—Ç–º–µ–Ω—ã
    builder.button(text="¬´ –û—Ç–º–µ–Ω–∞", callback_data="analyze_cancel")
    builder.adjust(2)  # 2 –∫–Ω–æ–ø–∫–∏ –≤ —Ä—è–¥—É
    
    return builder.as_markup()


@router.message(Command("analyze"))
async def cmd_analyze(message: Message, state: FSMContext) -> None:
    """Activate document analysis mode - now with prompt selection."""
    logger.info(f"User {message.from_user.id} activated /analyze")
    await start_analyze_mode(message=message, state=state)


async def start_analyze_mode(callback: CallbackQuery = None, message: Message = None, state: FSMContext = None) -> None:
    """Start interactive document analysis mode.
    
    NEW: Show prompt selection FIRST, then ask for document.
    –¢–û–õ–¨–ö–û –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤!
    """
    if state is None:
        logger.error("state is None in start_analyze_mode")
        return
    
    user_id = message.from_user.id if message else callback.from_user.id if callback else None
    
    if not user_id:
        logger.error("Cannot determine user_id")
        return
    
    # Load user prompts
    prompt_manager.load_user_prompts(user_id)
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–ª—É—á–∞–µ–º –¢–û–õ–¨–ö–û –¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    prompts = prompt_manager.get_prompt_by_category(user_id, "document_analysis")
    
    await state.set_state(ConversationStates.selecting_prompt)
    
    text = (
        "üìì *–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤*\n\n"
        "–®–∞–≥ 1‚ÅÖ3‚ÅÖ3‚ÅÖ –∏–∑ 2: *–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –∞–Ω–∞–ª–∏–∑–∞*\n\n"
        f"üìÑ *–î–æ—Å—Ç—É–ø–Ω–æ: {len(prompts)} –ø—Ä–æ–º–ø—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞*\n\n"
        "üîô *–ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:*\n"
        "1‚ÅÖ3‚ÅÖ3 –í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–æ–º–ø—Ç (—Ç–∏–ø –∞–Ω–∞–ª–∏–∑–∞)\n"
        "2‚ÅÖ3‚ÅÖ3 –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç\n"
        "3‚ÅÖ3‚ÅÖ3 –ü–æ–ª—É—á–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n\n"
        "‚úçÃ£ *–ö–∞–∫ –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–º–ø—Ç:*\n"
        "`/prompts` ‚Üí –î–æ–∫—É–º–µ–Ω—Ç—ã ‚Üí [–í—ã–±—Ä–∞—Ç—å] ‚Üí –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å\n\n"
        "üëá –ù–∏–∂–µ –≤—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –∞–Ω–∞–ª–∏–∑–∞:"
    )
    
    if message:
        await message.answer(
            text,
            parse_mode="Markdown",
            reply_markup=_get_prompts_keyboard(user_id),
        )
        logger.info(f"Analysis mode started for user {user_id} with {len(prompts)} document prompts")
    elif callback:
        await callback.message.answer(
            text,
            parse_mode="Markdown",
            reply_markup=_get_prompts_keyboard(user_id),
        )
        await callback.answer()
        logger.info(f"Analysis mode started for user {user_id} with {len(prompts)} document prompts")


@router.callback_query(F.data.startswith("analyze_select_prompt_"))
async def cb_select_prompt(query: CallbackQuery, state: FSMContext) -> None:
    """Handle prompt selection - move to document upload state."""
    prompt_name = query.data.replace("analyze_select_prompt_", "")
    user_id = query.from_user.id
    
    # Verify prompt exists
    prompt = prompt_manager.get_prompt(user_id, prompt_name)
    if not prompt:
        await query.answer("‚ùå –ü—Ä–æ–º–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω", show_alert=True)
        return
    
    # Save prompt to state
    await state.update_data(selected_prompt_name=prompt_name)
    
    # Move to document upload state
    await state.set_state(ConversationStates.ready)
    
    logger.info(f"User {user_id} selected prompt: {prompt_name}")
    
    text = (
        f"‚úÖ *–ü—Ä–æ–º–ø—Ç –≤—ã–±—Ä–∞–Ω!*\n\n"
        f"üìÑ *–¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞:* `{prompt_name}`\n"
        f"_{prompt.description}_\n\n"
        f"üìÇ *–®–∞–≥ 2‚ÅÖ3‚ÅÖ3 –∏–∑ 2:* –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç\n\n"
        f"üìÑ *–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:*\n"
        f"‚Ä¢ PDF, DOCX, TXT\n"
        f"‚Ä¢ Excel (.xlsx, .xls)\n"
        f"‚Ä¢ ZIP, DOC\n"
        f"‚Ä¢ üìá –§–æ—Ç–æ\n\n"
        f"‚úçÃ£ *–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –ø—Ä–æ–º–ø—Ç?*\n"
        f"`/prompts` ‚Üí –î–æ–∫—É–º–µ–Ω—Ç—ã ‚Üí `{prompt_name}` ‚Üí –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å\n\n"
        f"üìÅ –ì–æ—Ç–æ–≤–æ? –û—Ç–ø—Ä–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç!"
    )
    
    await query.message.edit_text(
        text,
        parse_mode="Markdown",
        reply_markup=InlineKeyboardMarkup(
            inline_keyboard=[[InlineKeyboardButton(text="¬´ –û—Ç–º–µ–Ω–∞", callback_data="analyze_back_to_prompts")]]
        ),
    )
    
    await query.answer()


@router.callback_query(F.data == "analyze_back_to_prompts")
async def cb_back_to_prompts(query: CallbackQuery, state: FSMContext) -> None:
    """Go back to prompt selection."""
    user_id = query.from_user.id
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–ª—É—á–∞–µ–º –¢–û–õ–¨–ö–û –¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    prompts = prompt_manager.get_prompt_by_category(user_id, "document_analysis")
    
    text = (
        "üìì *–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤*\n\n"
        "–®–∞–≥ 1‚ÅÖ3‚ÅÖ3 –∏–∑ 2: *–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –∞–Ω–∞–ª–∏–∑–∞*\n\n"
        f"üìÑ *–î–æ—Å—Ç—É–ø–Ω–æ: {len(prompts)} –ø—Ä–æ–º–ø—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞*\n\n"
        "‚úçÃ£ *–ö–∞–∫ –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–º–ø—Ç:*\n"
        "`/prompts` ‚Üí –î–æ–∫—É–º–µ–Ω—Ç—ã ‚Üí [–í—ã–±—Ä–∞—Ç—å] ‚Üí –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å\n\n"
        "üëá –ù–∏–∂–µ –≤—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –∞–Ω–∞–ª–∏–∑–∞:"
    )
    
    await state.set_state(ConversationStates.selecting_prompt)
    await query.message.edit_text(
        text,
        parse_mode="Markdown",
        reply_markup=_get_prompts_keyboard(user_id),
    )
    await query.answer()


@router.callback_query(F.data == "analyze_cancel")
async def cb_analyze_cancel(query: CallbackQuery, state: FSMContext) -> None:
    """Cancel analyze mode."""
    await state.clear()
    
    text = "‚ùå *–û—Ç–º–µ–Ω–µ–Ω–æ*\n\n–í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –≤ —Ä–µ–∂–∏–º –¥–∏–∞–ª–æ–≥–∞."
    
    await query.message.edit_text(
        text,
        parse_mode="Markdown",
    )
    await query.answer()
    logger.info(f"User {query.from_user.id} cancelled analyze mode")


@router.message(
    ConversationStates.ready,
    F.document
)
async def handle_document_upload(message: Message, state: FSMContext) -> None:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    
    –ê–†–•–ò–¢–ï–ö–¢–£–†–ù–û:
    –≠—Ç–æ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¢–û–õ–¨–ö–û –∫–æ–≥–¥–∞:
    1. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ç–æ—á–Ω–æ –≤ ConversationStates.ready
    2. –§–∏–ª—å—Ç—Ä –≤ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —ç—Ç–æ
    3. –ù–∏–∫–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –¥—Ä—É–≥–∏—Ö —Ä–µ–∂–∏–º–æ–≤ —Å—é–¥–∞ –Ω–µ –ø–æ–ø–∞–¥—É—Ç
    """
    if not message.document:
        await message.answer("‚ùå –î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    document: Document = message.document
    file_size = document.file_size or 0
    
    logger.info(f"User {message.from_user.id} uploading document in analyze mode: {document.file_name} ({file_size} bytes)")
    
    # Validate file size
    if file_size > config.MAX_FILE_SIZE:
        max_size_mb = config.MAX_FILE_SIZE / (1024 * 1024)
        await message.answer(
            f"‚ö†Ô∏è –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {file_size / (1024 * 1024):.1f} MB\n"
            f"–ú–∞–∫—Å–∏–º—É–º: {max_size_mb:.1f} MB"
        )
        return
    
    # Show processing
    status_msg = await message.answer(
        "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –¥–æ–∫—É–º–µ–Ω—Ç...\n"
        "–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞..."
    )
    
    # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï: –ö–∞–∂–¥—ã–π —Ñ–∞–π–ª –ø–æ–ª—É—á–∞–µ—Ç —Å–≤–æ–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π temp-–∫–∞—Ç–∞–ª–æ–≥
    file_uuid = str(uuid.uuid4())
    temp_user_dir = None
    
    try:
        # Create UNIQUE temp directory –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
        temp_base = Path(config.TEMP_DIR)
        temp_base.mkdir(exist_ok=True)
        
        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥: temp\{user_id}_{file_uuid}
        unique_temp_name = f"{message.from_user.id}_{file_uuid}"
        temp_user_dir = CleanupManager.create_temp_directory(
            temp_base,
            unique_temp_name,
        )
        
        # Download file
        bot = message.bot
        file: File = await bot.get_file(document.file_id)
        
        if not file.file_path:
            await message.answer("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É")
            await status_msg.delete()
            return
        
        # Generate unique filename
        file_ext = Path(document.file_name or "document").suffix or ".bin"
        temp_file_path = temp_user_dir / f"{file_uuid}{file_ext}"
        
        await bot.download_file(file.file_path, temp_file_path)
        logger.info(f"Downloaded: {temp_file_path} ({file_size} bytes)")
        
        # Extract text
        await status_msg.edit_text(
            "üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –¥–æ–∫—É–º–µ–Ω—Ç...\n"
            "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞..."
        )
        
        converter = FileConverter()
        extracted_text = converter.extract_text(temp_file_path, temp_user_dir)
        
        if not extracted_text or not extracted_text.strip():
            await message.answer(
                "‚ö†Ô∏è –í –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω —Ç–µ–∫—Å—Ç.\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ñ–∞–π–ª."
            )
            await status_msg.delete()
            return
        
        # Save to state - –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        await state.update_data(
            document_text=extracted_text,
            document_name=document.file_name or "document",
            document_size=len(extracted_text),
            user_id=message.from_user.id,
        )
        
        # Get prompt info from state
        data = await state.get_data()
        selected_prompt_name = data.get("selected_prompt_name", "default")
        
        # Log BEFORE analysis starts
        logger.info(
            f"Document loaded for user {message.from_user.id}: "
            f"{len(extracted_text)} chars with prompt '{selected_prompt_name}'"
        )
        
        # Update status message with analysis start (NO PREVIEW MESSAGE)
        await status_msg.edit_text(
            f"‚è≥ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å –ø—Ä–æ–º–ø—Ç–æ–º '{selected_prompt_name}'...\n"
            "–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è..."
        )
        
        # Immediately start analysis with selected prompt
        await _perform_analysis(message, state, data, status_msg)
    
    except ValueError as e:
        # Unsupported format
        logger.error(f"Error processing document: {e}")
        await message.answer(
            f"‚ö†Ô∏è {str(e)}\n\n"
            f"üìÑ *–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:*\n"
            f"‚Ä¢ PDF, DOCX, TXT\n"
            f"‚Ä¢ Excel (.xlsx, .xls)\n"
            f"‚Ä¢ ZIP\n\n"
            f"‚ùå *–ù–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è:* .doc (—Å—Ç–∞—Ä—ã–π Word)\n"
            f"–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–π—Ç–µ –≤ .docx –∏–ª–∏ PDF.",
            parse_mode="Markdown",
        )
        await status_msg.delete()
    except Exception as e:
        logger.error(f"Error processing document: {e}")
        await message.answer(f"‚ùå –û—à–∏–±–∫–∞: {str(e)[:80]}")
        await status_msg.delete()
    
    finally:
        # Cleanup ONLY this file's directory
        if temp_user_dir and temp_user_dir.exists():
            await CleanupManager.cleanup_directory_async(temp_user_dir)


@router.message(
    ConversationStates.ready,
    F.photo
)
async def handle_photo_upload(message: Message, state: FSMContext) -> None:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–æ—Ç–æ.
    
    –ê–†–•–ò–¢–ï–ö–¢–£–†–ù–û:
    –≠—Ç–æ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¢–û–õ–¨–ö–û –∫–æ–≥–¥–∞:
    1. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ç–æ—á–Ω–æ –≤ ConversationStates.ready
    2. –§–∏–ª—å—Ç—Ä –≤ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —ç—Ç–æ
    3. –ù–∏–∫–∞–∫–∏–µ —Ñ–æ—Ç–æ –∏–∑ –¥—Ä—É–≥–∏—Ö —Ä–µ–∂–∏–º–æ–≤ —Å—é–¥–∞ –Ω–µ –ø–æ–ø–∞–¥—É—Ç
    """
    if not message.photo:
        await message.answer("‚ùå –§–æ—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return
    
    logger.info(f"User {message.from_user.id} uploading photo in analyze mode")
    
    # Show processing ONLY - no confirmation message after
    status_msg = await message.answer(
        "‚è≥ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–æ—Ç–æ...\n"
        "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (OCR)..."
    )
    
    # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï: –û–Ω–∏–∫–∞–ª—å–Ω—ã–π temp-–∫–∞—Ç–∞–ª–æ–≥ –¥–ª—è —Ñ–æ—Ç–æ
    file_uuid = str(uuid.uuid4())
    temp_user_dir = None
    
    try:
        # Create UNIQUE temp directory
        temp_base = Path(config.TEMP_DIR)
        temp_base.mkdir(exist_ok=True)
        
        unique_temp_name = f"{message.from_user.id}_{file_uuid}"
        temp_user_dir = CleanupManager.create_temp_directory(
            temp_base,
            unique_temp_name,
        )
        
        # Extract text from photo using OCR
        extracted_text = await _extract_text_from_photo_for_analysis(message, temp_user_dir)
        
        if not extracted_text or not extracted_text.strip():
            await message.answer(
                "‚ö†Ô∏è –¢–µ–∫—Å—Ç –≤ —Ñ–æ—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω.\n"
                "–£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ:\n"
                "‚Ä¢ –§–æ—Ç–æ —á–µ—Ç–∫–æ–µ\n"
                "‚Ä¢ –¢–µ–∫—Å—Ç —Ö–æ—Ä–æ—à–æ –≤–∏–¥–µ–Ω\n"
                "‚Ä¢ –ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–π —Ñ–æ–Ω"
            )
            await status_msg.delete()
            return
        
        # Save to state - –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        await state.update_data(
            document_text=extracted_text,
            document_name="photo_document",
            document_size=len(extracted_text),
            user_id=message.from_user.id,
        )
        
        # Get prompt info from state
        data = await state.get_data()
        selected_prompt_name = data.get("selected_prompt_name", "default")
        
        # Log BEFORE analysis starts
        logger.info(
            f"Photo loaded for user {message.from_user.id}: "
            f"{len(extracted_text)} chars with prompt '{selected_prompt_name}'"
        )
        
        # Update status message with analysis start - NO "photo ready" message
        await status_msg.edit_text(
            f"‚è≥ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å –ø—Ä–æ–º–ø—Ç–æ–º '{selected_prompt_name}'...\n"
            "–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è..."
        )
        
        # Immediately start analysis with selected prompt
        await _perform_analysis(message, state, data, status_msg)
    
    except Exception as e:
        logger.error(f"Error processing photo: {e}")
        await message.answer(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        await status_msg.delete()
    
    finally:
        # Cleanup ONLY this photo's directory
        if temp_user_dir and temp_user_dir.exists():
            await CleanupManager.cleanup_directory_async(temp_user_dir)


async def _perform_analysis(
    message: Message, 
    state: FSMContext, 
    data: dict,
    status_msg: Message = None,
) -> None:
    """Perform analysis with selected prompt. Auto-delete progress message after sending results.
    
    IMPORTANT: After analysis completes, returns user to chat mode (clears state).
    This ensures they don't stay in analysis mode.
    """
    document_text = data.get("document_text")
    document_name = data.get("document_name", "document")
    selected_prompt_name = data.get("selected_prompt_name", "default")
    user_id = message.from_user.id
    
    if not document_text:
        await message.answer("‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω.")
        if status_msg:
            await status_msg.delete()
        # Return to chat mode
        await state.clear()
        return
    
    logger.info(
        f"User {user_id} starting analysis with prompt '{selected_prompt_name}'"
    )
    
    # Show typing
    await message.bot.send_chat_action(message.chat.id, "typing")
    
    try:
        # Get selected prompt
        prompt = prompt_manager.get_prompt(user_id, selected_prompt_name)
        
        if not prompt:
            prompt = prompt_manager.get_prompt(user_id, "default")
        
        if not prompt:
            await message.answer(
                "‚ùå –ü—Ä–æ–º–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. –û—Ç–∫—Ä—ã–≤–∞—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π..."
            )
        
        # Build analysis command
        analysis_command = prompt.user_prompt_template if prompt else "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –∫–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã."
        
        # Analyze with user_id for logging
        analysis_result = await llm_factory.analyze_document(
            document_text,
            analysis_command,
            system_prompt=prompt.system_prompt if prompt else None,
            use_streaming=False,
            user_id=user_id,
        )
        
        if not analysis_result:
            await message.answer("‚ùå –ê–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")
            if status_msg:
                await status_msg.delete()
            # Return to chat mode
            await state.clear()
            return
        
        # Split and send
        splitter = TextSplitter(max_length=4000)
        chunks = splitter.split(analysis_result)
        
        # –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï: –¥–æ–±–∞–≤–ª—è–µ–º –∏–º—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ –ù–ê–ß–ê–õ–û
        if len(chunks) == 1:
            # –û–¥–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ
            header = f"üìÑ *–î–æ–∫—É–º–µ–Ω—Ç:* `{document_name}`\n\n"
            await message.answer(
                f"{header}{analysis_result}",
                parse_mode="Markdown",
            )
        else:
            # –ù–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π - –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–æ–ª—å–∫–æ –≤ –ø–µ—Ä–≤–æ–º
            for i, chunk in enumerate(chunks, 1):
                if i == 1:
                    # –ü–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –∏ –Ω–æ–º–µ—Ä–æ–º
                    prefix = f"üìÑ *–î–æ–∫—É–º–µ–Ω—Ç:* `{document_name}`\n\n*[–ß–∞—Å—Ç—å {i}/{len(chunks)}]*\n\n"
                else:
                    # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                    prefix = f"*[–ß–∞—Å—Ç—å {i}/{len(chunks)}]*\n\n"
                
                await message.answer(
                    f"{prefix}{chunk}",
                    parse_mode="Markdown",
                )
        
        # Delete progress message after results sent
        if status_msg:
            await status_msg.delete()
        
        logger.info(
            f"Analysis completed for user {user_id}: "
            f"{len(analysis_result)} chars in {len(chunks)} parts"
        )
        
        # CRITICAL: Return to chat mode after analysis completes
        logger.info(f"User {user_id} returned to chat mode")
        await state.clear()
    
    except Exception as e:
        logger.error(f"Analysis error: {e}")
        await message.answer(f"‚ùå –û—à–∏–±–∫–∞: {str(e)[:100]}")
        if status_msg:
            await status_msg.delete()
        # Return to chat mode even on error
        await state.clear()


async def _extract_text_from_photo_for_analysis(
    message: Message,
    temp_dir: Path,
) -> str:
    """Extract text from photo using OCR.space cloud API.
    
    Args:
        message: Message with photo
        temp_dir: Temporary directory
        
    Returns:
        Extracted text from photo
    """
    try:
        import httpx
        import base64
        import asyncio
        
        logger.info(f"OCR: Starting extraction for user {message.from_user.id}")
        
        # Get largest photo
        if not message.photo:
            logger.warning("OCR: No photo found in message")
            return ""
        
        photo = message.photo[-1]
        logger.info(f"OCR: Got photo {photo.file_id}, size: {photo.file_size} bytes")
        
        # Get file info
        file_info = await message.bot.get_file(photo.file_id)
        logger.info(f"OCR: File path: {file_info.file_path}")
        
        # Download photo
        temp_file = temp_dir / f"photo_{photo.file_unique_id}.jpg"
        logger.info(f"OCR: Downloading to {temp_file}")
        await message.bot.download_file(file_info.file_path, temp_file)
        logger.info(f"OCR: Downloaded successfully, size: {temp_file.stat().st_size} bytes")
        
        # Read photo as base64
        with open(temp_file, "rb") as f:
            photo_bytes = f.read()
        logger.info(f"OCR: Read {len(photo_bytes)} bytes from file")
        
        photo_base64 = base64.b64encode(photo_bytes).decode("utf-8")
        logger.info(f"OCR: Encoded to base64, size: {len(photo_base64)} chars")
        
        # Prepare API payload
        api_key = config.OCR_SPACE_API_KEY
        if not api_key:
            logger.error("OCR: OCR_SPACE_API_KEY not configured")
            return ""
        
        payload = {
            "apikey": api_key,
            "base64Image": f"data:image/jpeg;base64,{photo_base64}",
            "language": "rus",
            "isOverlayRequired": False,
            "detectOrientation": True,
            "scale": True,
            "OCREngine": 2,
        }
        logger.info(f"OCR: Prepared payload, base64 size: {len(payload['base64Image'])} chars")
        
        # Call OCR.space API with proper timeouts
        logger.info("OCR: Calling OCR.space API...")
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.ocr.space/parse/image",
                data=payload,
                timeout=httpx.Timeout(60.0, connect=30.0),
            )
            
            logger.info(f"OCR: Got response status {response.status_code}")
            
            if response.status_code != 200:
                logger.error(f"OCR: API error {response.status_code}: {response.text[:200]}")
                return ""
            
            # Parse response
            try:
                result = response.json()
            except Exception as e:
                logger.error(f"OCR: Failed to parse JSON response: {e}")
                logger.error(f"OCR: Response text: {response.text[:500]}")
                return ""
            
            logger.info(f"OCR: Response keys: {result.keys()}")
            
            # Check for processing errors
            if result.get("IsErroredOnProcessing"):
                error_msg = result.get("ErrorMessage", "Unknown error")
                logger.error(f"OCR: Processing error: {error_msg}")
                return ""
            
            # Extract text from parsed results
            parsed_results = result.get("ParsedResults", [])
            if not parsed_results:
                logger.warning("OCR: No parsed results in response")
                logger.info(f"OCR: Full response: {result}")
                return ""
            
            text = parsed_results[0].get("ParsedText", "")
            logger.info(f"OCR: Successfully extracted {len(text)} chars from photo")
            return text.strip()
    
    except asyncio.TimeoutError:
        logger.error("OCR: Request timeout (60s exceeded)")
        return ""
    except Exception as e:
        logger.error(f"OCR: Exception during extraction: {type(e).__name__}: {e}")
        import traceback
        logger.error(f"OCR: Traceback:\n{traceback.format_exc()}")
        return ""


# Legacy callbacks - not used in new design
@router.callback_query(F.data == "doc_clear")
async def cb_doc_clear(query: CallbackQuery, state: FSMContext) -> None:
    """Clear document (legacy)."""
    await state.clear()
    await state.set_state(ConversationStates.ready)
    await query.message.answer("üóëÔ∏è –î–æ–∫—É–º–µ–Ω—Ç –æ—á–∏—â–µ–Ω. –ó–∞–≥—Ä—É–∂–∞–π—Ç–µ –Ω–æ–≤—ã–π.")
    await query.answer()


@router.callback_query(F.data == "doc_info")
async def cb_doc_info(query: CallbackQuery, state: FSMContext) -> None:
    """Show doc info (legacy)."""
    data = await state.get_data()
    document_name = data.get("document_name", "Unknown")
    document_size = data.get("document_size", 0)
    
    text = (
        f"üìã *–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ*\n\n"
        f"*–ò–º—è:* `{document_name}`\n"
        f"*–†–∞–∑–º–µ—Ä:* {document_size:,} —Å–∏–º–≤–æ–ª–æ–≤"
    )
    
    await query.message.answer(text, parse_mode="Markdown")
    await query.answer()

"""Chat mode handlers for simple AI conversation.

Fixes 2025-12-20 19:00:
- Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð° ÑÐ²Ð½Ð°Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð”Ðž Ð°ÐºÑ‚Ð¸Ð²Ð°Ñ†Ð¸Ð¸ Ñ‡Ð°Ñ‚Ð°
- Ð—Ð°Ñ‰Ð¸Ñ‚Ð° Ð¾Ñ‚ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ ÐµÑÐ»Ð¸ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð½Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾
- ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ‡Ñ‚Ð¾ Ð¼Ñ‹ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð² ChatStates.chatting Ð¿ÐµÑ€ÐµÐ´ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¾Ð¹

Fixes 2025-12-20 17:09:
- Now uses manageable chat_system prompt from PromptManager
- Users can edit chat prompt via /prompts > Dialog
- Falls back to system default if user hasn't customized
- Loads user prompts on each message

Allows users to have a normal conversation with AI without
needing to upload documents. This is the DEFAULT mode.
"""

import logging
from aiogram import Router, F
from aiogram.filters import Command
from aiogram.types import Message, CallbackQuery
from aiogram.fsm.context import FSMContext

from app.config import get_settings
from app.localization import ru
from app.states.chat import ChatStates
from app.services.llm.llm_factory import LLMFactory
from app.services.prompts.prompt_manager import PromptManager
from app.utils.text_splitter import TextSplitter

logger = logging.getLogger(__name__)

router = Router()
config = get_settings()
llm_factory = LLMFactory(
    primary_provider=config.LLM_PROVIDER,
    openai_api_key=config.OPENAI_API_KEY or None,
    openai_model=config.OPENAI_MODEL,
    replicate_api_token=config.REPLICATE_API_TOKEN or None,
    replicate_model=config.REPLICATE_MODEL,
)
prompt_manager = PromptManager()


@router.message(Command("chat"))
async def cmd_chat(message: Message, state: FSMContext) -> None:
    """Activate chat mode explicitly.
    
    Note: Chat mode is active by default after /start.
    This command just confirms it.
    
    Ð’ÐÐ–ÐÐž: Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¾Ñ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ, Ð¿Ð¾Ñ‚Ð¾Ð¼ ÑƒÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð¾Ð²Ð¾Ðµ
    Ð­Ñ‚Ð¾ Ð¿Ñ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÐºÐ¾Ð½Ñ„Ð»Ð¸ÐºÑ‚Ñ‹ Ñ Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸ Ñ€ÐµÐ¶Ð¸Ð¼Ð°Ð¼Ð¸ (homework, analyze Ð¸ Ñ‚.Ð´.)
    """
    # Ð¨Ð°Ð³ 1: ÐŸÐžÐ›ÐÐÐ¯ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ð²ÑÐµÑ… Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ñ… ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¹
    await state.clear()
    logger.debug(f"Cleared state for user {message.from_user.id}")
    
    # Ð¨Ð°Ð³ 2: Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ñ‡Ð°Ñ‚Ð° ÐŸÐžÐ¡Ð›Ð• Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸
    await state.set_state(ChatStates.chatting)
    logger.debug(f"Set ChatStates.chatting for user {message.from_user.id}")
    
    text = (
        "ðŸ’¬ *Ð ÐµÐ¶Ð¸Ð¼ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð° Ð°ÐºÑ‚Ð¸Ð²ÐµÐ½*\n\n"
        "ÐŸÐ¸ÑˆÐ¸Ñ‚Ðµ Ð¼Ð½Ðµ ÑÐ²Ð¾Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ñ Ð³Ð¾Ñ‚Ð¾Ð² Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ!"
    )
    
    await message.answer(
        text,
        parse_mode="Markdown",
    )
    
    logger.info(f"Chat mode activated for user {message.from_user.id}")


async def start_chat_mode(callback: CallbackQuery = None, message: Message = None, state: FSMContext = None) -> None:
    """Start chat mode (legacy function for compatibility).
    
    Ð’ÐÐ–ÐÐž: Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¾Ñ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ, Ð¿Ð¾Ñ‚Ð¾Ð¼ ÑƒÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð¾Ð²Ð¾Ðµ
    """
    if state is None:
        logger.error("state is None in start_chat_mode")
        return
    
    # Ð¨Ð°Ð³ 1: ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ñ… ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¹
    await state.clear()
    
    # Ð¨Ð°Ð³ 2: Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ñ‡Ð°Ñ‚Ð°
    await state.set_state(ChatStates.chatting)
    
    text = (
        "ðŸ’¬ *Ð ÐµÐ¶Ð¸Ð¼ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð° Ð°ÐºÑ‚Ð¸Ð²ÐµÐ½*\n\n"
        "ÐŸÐ¸ÑˆÐ¸Ñ‚Ðµ Ð¼Ð½Ðµ ÑÐ²Ð¾Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ñ Ð³Ð¾Ñ‚Ð¾Ð² Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ!"
    )
    
    if message:
        await message.answer(
            text,
            parse_mode="Markdown",
        )
    elif callback:
        await callback.message.answer(
            text,
            parse_mode="Markdown",
        )
        await callback.answer()
    
    logger.info("Chat mode started")


@router.message(ChatStates.chatting, F.text)
async def handle_chat_message(message: Message, state: FSMContext) -> None:
    """Handle user message in chat mode.
    
    ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº Ð¢ÐžÐ›Ð¬ÐšÐž ÑÑ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ ÐµÑÐ»Ð¸ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ñ‚Ð¾Ñ‡Ð½Ð¾ ChatStates.chatting
    Ð‘Ð»Ð°Ð³Ð¾Ð´Ð°Ñ€Ñ aiogram FSM, ÑÑ‚Ð¾ Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ñ‡Ñ‚Ð¾ Ð¼Ñ‹ Ð½Ðµ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ
    Ð¸Ð· Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ñ€ÐµÐ¶Ð¸Ð¼Ð¾Ð² (homework, analyze Ð¸ Ñ‚.Ð´.)
    
    Process the message and respond with AI using manageable prompt.
    """
    user_message = message.text.strip()
    user_id = message.from_user.id
    
    if not user_message:
        await message.answer(ru.CHAT_EMPTY)
        return
    
    # Skip commands
    if user_message.startswith("/"):
        return
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‡Ñ‚Ð¾ Ð¼Ñ‹ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð² Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¸
    current_state = await state.get_state()
    if current_state != ChatStates.chatting.state:
        logger.warning(
            f"User {user_id} sent message but not in chat state. "
            f"Current state: {current_state}"
        )
        await state.set_state(ChatStates.chatting)
    
    # Load user prompts to get custom chat_system if exists
    prompt_manager.load_user_prompts(user_id)
    
    # Get chat system prompt (from user custom or default)
    chat_prompt = prompt_manager.get_prompt(user_id, "chat_system")
    if not chat_prompt:
        logger.warning(f"Chat prompt not found for user {user_id}, using default")
        system_prompt = (
            "ÐŸÐ¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº Ð´Ð»Ñ Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ñ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ñ‹Ñ… Ñ‚ÐµÐ¼. "
            "ÐÐ° Ð²ÐµÑÑŒ Ñ€ÑƒÑÑÐºÐ¾Ð¼. "
            "Ð‘ÑƒÐ´ÑŒ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ñ‹Ð¼, Ð±ÑƒÐ´ÑŒ Ð¿Ð¾Ð»Ð½Ñ‹Ð¼, Ð±ÑƒÐ´ÑŒ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¼."
        )
    else:
        system_prompt = chat_prompt.system_prompt
    
    # Show "typing..." indicator AND status message
    await message.bot.send_chat_action(message.chat.id, "typing")
    
    # Send visual progress message
    progress_msg = await message.answer(
        "ðŸ¤” Ð”ÑƒÐ¼Ð°ÑŽ Ð½Ð°Ð´ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð¼...",
        parse_mode="Markdown",
    )
    
    try:
        # Generate response from LLM
        response = await llm_factory.chat(
            user_message=user_message,
            system_prompt=system_prompt,
            use_streaming=False,
        )
        
        # Delete progress message
        await progress_msg.delete()
        
        if not response:
            await message.answer(ru.CHAT_ERROR)
            return
        
        # Split long messages (Telegram limit is 4096 chars)
        splitter = TextSplitter(max_length=4000)
        chunks = splitter.split(response)
        
        # Send response in chunks
        if len(chunks) == 1:
            # Single message
            try:
                await message.answer(
                    response,
                    parse_mode="Markdown",
                )
            except Exception as e:
                # If markdown fails, send as plain text
                logger.warning(f"Markdown failed: {e}, sending as plain text")
                await message.answer(response)
        else:
            # Multiple messages
            for i, chunk in enumerate(chunks, 1):
                try:
                    prefix = f"*[Ð§Ð°ÑÑ‚ÑŒ {i}/{len(chunks)}]*\n\n"
                    await message.answer(
                        f"{prefix}{chunk}",
                        parse_mode="Markdown",
                    )
                except Exception as e:
                    # If markdown fails, send as plain text
                    logger.warning(f"Markdown failed: {e}, sending as plain text")
                    await message.answer(f"[Ð§Ð°ÑÑ‚ÑŒ {i}/{len(chunks)}]\n\n{chunk}")
        
        logger.info(f"Chat response: {len(response)} chars in {len(chunks)} messages")
    
    except Exception as e:
        logger.error(f"Chat error: {e}")
        # Delete progress message on error
        try:
            await progress_msg.delete()
        except:
            pass
        
        await message.answer(
            f"{ru.CHAT_ERROR}\n\nÐžÑˆÐ¸Ð±ÐºÐ°: {str(e)[:100]}",
        )


@router.callback_query(F.data == "chat_exit")
async def cb_chat_exit(callback: CallbackQuery, state: FSMContext) -> None:
    """Exit chat mode (legacy - not used in new design)."""
    from app.handlers.common import cb_back_to_main
    await cb_back_to_main(callback, state)
